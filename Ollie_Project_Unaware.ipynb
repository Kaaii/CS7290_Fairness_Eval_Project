{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from torch import nn\n",
    "from pyro.nn import PyroModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exo_dist = {\n",
    "    'Nr': dist.Bernoulli(torch.tensor(0.7)),\n",
    "    'Ns': dist.Bernoulli(torch.tensor(0.35)),\n",
    "    'Na': dist.Normal(torch.tensor(0.), torch.tensor(1.))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(exo_dist):\n",
    "    # sample from bernoulli 0 or 1, 0 at 70% freq (made up)\n",
    "    R = pyro.sample(\"R\", exo_dist['Nr'])\n",
    "    S = pyro.sample(\"S\", exo_dist['Ns'])\n",
    "    \n",
    "    # random gaussian dist for ability \n",
    "    A = pyro.sample(\"A\", exo_dist['Na'])\n",
    "    \n",
    "    \n",
    "    G = pyro.sample(\"G\", dist.Normal(A + 2.1 * R + 3.3 * S, 0.5))\n",
    "    \n",
    "    L = pyro.sample(\"L\", dist.Normal(A + 5.8*R + 0.7*S, 0.1))\n",
    "    \n",
    "    F = pyro.sample(\"F\", dist.Normal(A + 2.3*R + 1.*S, 0.3))\n",
    "\n",
    "trace_handler = pyro.poutine.trace(model)\n",
    "samples = pd.DataFrame(columns=['R', 'S', 'A', 'G', 'L', 'F', 'p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unaware Model\n",
    "\n",
    "This model uses only G and L to predict FYA. It is indirectly biased because R/S affect G and L, and also affect F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>A</th>\n",
       "      <th>G</th>\n",
       "      <th>L</th>\n",
       "      <th>F</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensor(0.)</td>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(-0.2148)</td>\n",
       "      <td>tensor(3.1644)</td>\n",
       "      <td>tensor(0.4191)</td>\n",
       "      <td>tensor(0.7725)</td>\n",
       "      <td>tensor(0.1373)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(0.)</td>\n",
       "      <td>tensor(-1.8349)</td>\n",
       "      <td>tensor(-0.8424)</td>\n",
       "      <td>tensor(3.9694)</td>\n",
       "      <td>tensor(0.8623)</td>\n",
       "      <td>tensor(0.0051)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(0.2222)</td>\n",
       "      <td>tensor(5.9109)</td>\n",
       "      <td>tensor(6.5458)</td>\n",
       "      <td>tensor(3.5901)</td>\n",
       "      <td>tensor(0.0703)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(0.8138)</td>\n",
       "      <td>tensor(6.4073)</td>\n",
       "      <td>tensor(7.3679)</td>\n",
       "      <td>tensor(4.3132)</td>\n",
       "      <td>tensor(0.1909)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(0.5848)</td>\n",
       "      <td>tensor(6.0923)</td>\n",
       "      <td>tensor(7.0439)</td>\n",
       "      <td>tensor(3.8349)</td>\n",
       "      <td>tensor(0.3091)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            R           S                A                G               L  \\\n",
       "0  tensor(0.)  tensor(1.)  tensor(-0.2148)   tensor(3.1644)  tensor(0.4191)   \n",
       "1  tensor(1.)  tensor(0.)  tensor(-1.8349)  tensor(-0.8424)  tensor(3.9694)   \n",
       "2  tensor(1.)  tensor(1.)   tensor(0.2222)   tensor(5.9109)  tensor(6.5458)   \n",
       "3  tensor(1.)  tensor(1.)   tensor(0.8138)   tensor(6.4073)  tensor(7.3679)   \n",
       "4  tensor(1.)  tensor(1.)   tensor(0.5848)   tensor(6.0923)  tensor(7.0439)   \n",
       "\n",
       "                F               p  \n",
       "0  tensor(0.7725)  tensor(0.1373)  \n",
       "1  tensor(0.8623)  tensor(0.0051)  \n",
       "2  tensor(3.5901)  tensor(0.0703)  \n",
       "3  tensor(4.3132)  tensor(0.1909)  \n",
       "4  tensor(3.8349)  tensor(0.3091)  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unaware_sample= []\n",
    "for i in range(1000):\n",
    "    trace = trace_handler.get_trace(exo_dist)\n",
    "    R = trace.nodes['R']['value']\n",
    "    S = trace.nodes['S']['value']\n",
    "    A = trace.nodes['A']['value']\n",
    "    G = trace.nodes['G']['value']\n",
    "    L = trace.nodes['L']['value']\n",
    "    F = trace.nodes['F']['value']\n",
    "    # get prob of each combination\n",
    "    log_prob = trace.log_prob_sum()\n",
    "    p = np.exp(log_prob)\n",
    "    samples = samples.append({'R': R, 'S': S, 'A': A, 'G': G, 'L':L, 'F': F, 'p': p}, ignore_index=True)\n",
    "    unaware_sample.append(([G,L,F]))\n",
    "\n",
    "samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0010] loss: 1228.0948\n",
      "[iteration 0020] loss: 920.1564\n",
      "[iteration 0030] loss: 762.8047\n",
      "[iteration 0040] loss: 438.1056\n",
      "[iteration 0050] loss: 353.0892\n",
      "[iteration 0060] loss: 321.7837\n",
      "[iteration 0070] loss: 300.1779\n",
      "[iteration 0080] loss: 289.3389\n",
      "[iteration 0090] loss: 284.8181\n",
      "[iteration 0100] loss: 282.8357\n",
      "[iteration 0110] loss: 281.8677\n",
      "[iteration 0120] loss: 281.3944\n",
      "[iteration 0130] loss: 281.1844\n",
      "[iteration 0140] loss: 281.1036\n",
      "[iteration 0150] loss: 281.0769\n",
      "[iteration 0160] loss: 281.0694\n",
      "[iteration 0170] loss: 281.0674\n",
      "[iteration 0180] loss: 281.0671\n",
      "[iteration 0190] loss: 281.0671\n",
      "[iteration 0200] loss: 281.0671\n",
      "[iteration 0210] loss: 281.0671\n",
      "[iteration 0220] loss: 281.0671\n",
      "[iteration 0230] loss: 281.0671\n",
      "[iteration 0240] loss: 281.0671\n",
      "[iteration 0250] loss: 281.0671\n",
      "[iteration 0260] loss: 281.0671\n",
      "[iteration 0270] loss: 281.0671\n",
      "[iteration 0280] loss: 281.0671\n",
      "[iteration 0290] loss: 281.0671\n",
      "[iteration 0300] loss: 281.0671\n",
      "[iteration 0310] loss: 281.0671\n",
      "[iteration 0320] loss: 281.0671\n",
      "[iteration 0330] loss: 281.0671\n",
      "[iteration 0340] loss: 281.0671\n",
      "[iteration 0350] loss: 281.0671\n",
      "[iteration 0360] loss: 281.0671\n",
      "[iteration 0370] loss: 281.0671\n",
      "[iteration 0380] loss: 281.0671\n",
      "[iteration 0390] loss: 281.0671\n",
      "[iteration 0400] loss: 281.0671\n",
      "[iteration 0410] loss: 281.0671\n",
      "[iteration 0420] loss: 281.0671\n",
      "[iteration 0430] loss: 281.0671\n",
      "[iteration 0440] loss: 281.0671\n",
      "[iteration 0450] loss: 281.0671\n",
      "[iteration 0460] loss: 281.0671\n",
      "[iteration 0470] loss: 281.0671\n",
      "[iteration 0480] loss: 281.0671\n",
      "[iteration 0490] loss: 281.0671\n",
      "[iteration 0500] loss: 281.0671\n",
      "Learned parameters:\n",
      "weight [[0.27673978 0.35073876]]\n",
      "bias [-0.28901267]\n"
     ]
    }
   ],
   "source": [
    "# for CI testing\n",
    "smoke_test = ('CI' in os.environ)\n",
    "pyro.enable_validation(True)\n",
    "pyro.set_rng_seed(1)\n",
    "pyro.enable_validation(True)\n",
    "\n",
    "\n",
    "# setup\n",
    "assert issubclass(PyroModule[nn.Linear], nn.Linear)\n",
    "assert issubclass(PyroModule[nn.Linear], PyroModule)\n",
    "\n",
    "\n",
    "#Data to regress\n",
    "unaware_sample = torch.tensor(unaware_sample)\n",
    "x_data, y_data = unaware_sample[:, :-1], unaware_sample[:, -1]\n",
    "\n",
    "# Regression model\n",
    "# 2 = in features, 1=out feature\n",
    "linear_reg_model = PyroModule[nn.Linear](2, 1)\n",
    "\n",
    "# Define loss and optimize\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optim = torch.optim.Adam(linear_reg_model.parameters(), lr=0.05)\n",
    "num_iterations = 500 if not smoke_test else 2\n",
    "\n",
    "def train():\n",
    "    # run the model forward on the data\n",
    "    y_pred = linear_reg_model(x_data).squeeze(-1)\n",
    "    # calculate the mse loss\n",
    "    loss = loss_fn(y_pred, y_data)\n",
    "    # initialize gradients to zero\n",
    "    optim.zero_grad()\n",
    "    # backpropagate\n",
    "    loss.backward()\n",
    "    # take a gradient step\n",
    "    optim.step()\n",
    "    return loss\n",
    "\n",
    "for j in range(num_iterations):\n",
    "    loss = train()\n",
    "    if (j + 1) % 10 == 0:\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss.item()))\n",
    "\n",
    "# Inspect learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "for name, param in linear_reg_model.named_parameters():\n",
    "    print(name, param.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 17.,  45.,  56.,  47.,  38.,  81., 155., 137.,  40.,  14.]),\n",
       " array([-1.743669  , -1.1420765 , -0.5404841 ,  0.06110845,  0.66270095,\n",
       "         1.2642934 ,  1.865886  ,  2.4674785 ,  3.0690708 ,  3.6706634 ,\n",
       "         4.272256  ], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit = samples.copy()\n",
    "fit[\"mean\"] = linear_reg_model(x_data).detach().numpy()\n",
    "\n",
    "S1 = fit[fit[\"S\"] == 1]\n",
    "S0 = fit[fit[\"S\"] == 0]\n",
    "R1 = fit[fit[\"R\"] == 1]\n",
    "R0 = fit[fit[\"R\"] == 0]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n",
    "ax[0].hist(R1[\"mean\"])\n",
    "ax[0].hist(R0[\"mean\"])\n",
    "ax[1].hist(S1[\"mean\"])\n",
    "ax[1].hist(S0[\"mean\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Model\n",
    "\n",
    "This model uses all observable variables. This has the consequence of using R/S directly in its decision. \n",
    "(Causal model here would show R/S causing FYA as well). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>A</th>\n",
       "      <th>G</th>\n",
       "      <th>L</th>\n",
       "      <th>F</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(0.0739)</td>\n",
       "      <td>tensor(4.8844)</td>\n",
       "      <td>tensor(6.4200)</td>\n",
       "      <td>tensor(3.2143)</td>\n",
       "      <td>tensor(0.0547)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(0.)</td>\n",
       "      <td>tensor(1.6005)</td>\n",
       "      <td>tensor(3.9457)</td>\n",
       "      <td>tensor(7.4651)</td>\n",
       "      <td>tensor(4.0628)</td>\n",
       "      <td>tensor(0.1327)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(0.)</td>\n",
       "      <td>tensor(-0.6672)</td>\n",
       "      <td>tensor(1.1866)</td>\n",
       "      <td>tensor(5.1768)</td>\n",
       "      <td>tensor(1.9296)</td>\n",
       "      <td>tensor(0.3032)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(0.)</td>\n",
       "      <td>tensor(-0.2638)</td>\n",
       "      <td>tensor(2.5346)</td>\n",
       "      <td>tensor(5.3216)</td>\n",
       "      <td>tensor(1.7472)</td>\n",
       "      <td>tensor(0.0176)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(0.)</td>\n",
       "      <td>tensor(-0.9319)</td>\n",
       "      <td>tensor(1.6503)</td>\n",
       "      <td>tensor(4.8414)</td>\n",
       "      <td>tensor(0.9323)</td>\n",
       "      <td>tensor(0.1051)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            R           S                A               G               L  \\\n",
       "0  tensor(1.)  tensor(1.)   tensor(0.0739)  tensor(4.8844)  tensor(6.4200)   \n",
       "1  tensor(1.)  tensor(0.)   tensor(1.6005)  tensor(3.9457)  tensor(7.4651)   \n",
       "2  tensor(1.)  tensor(0.)  tensor(-0.6672)  tensor(1.1866)  tensor(5.1768)   \n",
       "3  tensor(1.)  tensor(0.)  tensor(-0.2638)  tensor(2.5346)  tensor(5.3216)   \n",
       "4  tensor(1.)  tensor(0.)  tensor(-0.9319)  tensor(1.6503)  tensor(4.8414)   \n",
       "\n",
       "                F               p  \n",
       "0  tensor(3.2143)  tensor(0.0547)  \n",
       "1  tensor(4.0628)  tensor(0.1327)  \n",
       "2  tensor(1.9296)  tensor(0.3032)  \n",
       "3  tensor(1.7472)  tensor(0.0176)  \n",
       "4  tensor(0.9323)  tensor(0.1051)  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = pd.DataFrame(columns=['R', 'S', 'A', 'G', 'L', 'F', 'p'])\n",
    "full_sample= []\n",
    "for i in range(1000):\n",
    "    trace = trace_handler.get_trace(exo_dist)\n",
    "    R = trace.nodes['R']['value']\n",
    "    S = trace.nodes['S']['value']\n",
    "    A = trace.nodes['A']['value']\n",
    "    G = trace.nodes['G']['value']\n",
    "    L = trace.nodes['L']['value']\n",
    "    F = trace.nodes['F']['value']\n",
    "    # get prob of each combination\n",
    "    log_prob = trace.log_prob_sum()\n",
    "    p = np.exp(log_prob)\n",
    "    samples=samples.append({'R': R, 'S': S, 'A': A, 'G': G, 'L':L, 'F': F, 'p': p}, ignore_index=True)\n",
    "    full_sample.append(([R,S,G,L,F]))\n",
    "\n",
    "samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0010] loss: 789.6496\n",
      "[iteration 0020] loss: 971.4789\n",
      "[iteration 0030] loss: 723.6378\n",
      "[iteration 0040] loss: 383.9923\n",
      "[iteration 0050] loss: 370.0474\n",
      "[iteration 0060] loss: 339.2785\n",
      "[iteration 0070] loss: 302.6099\n",
      "[iteration 0080] loss: 278.8698\n",
      "[iteration 0090] loss: 261.9639\n",
      "[iteration 0100] loss: 248.0414\n",
      "[iteration 0110] loss: 236.1470\n",
      "[iteration 0120] loss: 225.9137\n",
      "[iteration 0130] loss: 217.0389\n",
      "[iteration 0140] loss: 209.2750\n",
      "[iteration 0150] loss: 202.4385\n",
      "[iteration 0160] loss: 196.3949\n",
      "[iteration 0170] loss: 191.0415\n",
      "[iteration 0180] loss: 186.2945\n",
      "[iteration 0190] loss: 182.0818\n",
      "[iteration 0200] loss: 178.3382\n",
      "[iteration 0210] loss: 175.0039\n",
      "[iteration 0220] loss: 172.0241\n",
      "[iteration 0230] loss: 169.3489\n",
      "[iteration 0240] loss: 166.9330\n",
      "[iteration 0250] loss: 164.7365\n",
      "[iteration 0260] loss: 162.7241\n",
      "[iteration 0270] loss: 160.8655\n",
      "[iteration 0280] loss: 159.1345\n",
      "[iteration 0290] loss: 157.5092\n",
      "[iteration 0300] loss: 155.9714\n",
      "[iteration 0310] loss: 154.5060\n",
      "[iteration 0320] loss: 153.1007\n",
      "[iteration 0330] loss: 151.7456\n",
      "[iteration 0340] loss: 150.4330\n",
      "[iteration 0350] loss: 149.1566\n",
      "[iteration 0360] loss: 147.9118\n",
      "[iteration 0370] loss: 146.6950\n",
      "[iteration 0380] loss: 145.5035\n",
      "[iteration 0390] loss: 144.3354\n",
      "[iteration 0400] loss: 143.1892\n",
      "[iteration 0410] loss: 142.0640\n",
      "[iteration 0420] loss: 140.9592\n",
      "[iteration 0430] loss: 139.8745\n",
      "[iteration 0440] loss: 138.8097\n",
      "[iteration 0450] loss: 137.7646\n",
      "[iteration 0460] loss: 136.7394\n",
      "[iteration 0470] loss: 135.7341\n",
      "[iteration 0480] loss: 134.7488\n",
      "[iteration 0490] loss: 133.7836\n",
      "[iteration 0500] loss: 132.8387\n",
      "Learned parameters:\n",
      "weight [[-1.8048851  -0.45470715  0.30250278  0.61171335]]\n",
      "bias [-0.04319745]\n"
     ]
    }
   ],
   "source": [
    "#Data to regress\n",
    "full_sample = torch.tensor(full_sample)\n",
    "x_data, y_data = full_sample[:, :-1], full_sample[:, -1]\n",
    "\n",
    "# Regression model\n",
    "# 2 = in features, 1=out feature\n",
    "linear_reg_model = PyroModule[nn.Linear](4, 1)\n",
    "\n",
    "# Define loss and optimize\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optim = torch.optim.Adam(linear_reg_model.parameters(), lr=0.05)\n",
    "num_iterations = 500 if not smoke_test else 2\n",
    "\n",
    "def train():\n",
    "    # run the model forward on the data\n",
    "    y_pred = linear_reg_model(x_data).squeeze(-1)\n",
    "    # calculate the mse loss\n",
    "    loss = loss_fn(y_pred, y_data)\n",
    "    # initialize gradients to zero\n",
    "    optim.zero_grad()\n",
    "    # backpropagate\n",
    "    loss.backward()\n",
    "    # take a gradient step\n",
    "    optim.step()\n",
    "    return loss\n",
    "\n",
    "for j in range(num_iterations):\n",
    "    loss = train()\n",
    "    if (j + 1) % 10 == 0:\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss.item()))\n",
    "\n",
    "# Inspect learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "for name, param in linear_reg_model.named_parameters():\n",
    "    print(name, param.data.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.,  19.,  33.,  65.,  77., 134., 155.,  98.,  45.,   2.]),\n",
       " array([-2.8060572 , -2.0081537 , -1.2102501 , -0.41234654,  0.38555703,\n",
       "         1.1834606 ,  1.9813641 ,  2.7792678 ,  3.5771713 ,  4.375075  ,\n",
       "         5.1729784 ], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit = samples.copy()\n",
    "fit[\"mean\"] = linear_reg_model(x_data).detach().numpy()\n",
    "\n",
    "S1 = fit[fit[\"S\"] == 1]\n",
    "S0 = fit[fit[\"S\"] == 0]\n",
    "R1 = fit[fit[\"R\"] == 1]\n",
    "R0 = fit[fit[\"R\"] == 0]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n",
    "ax[0].hist(R1[\"mean\"])\n",
    "ax[0].hist(R0[\"mean\"])\n",
    "ax[1].hist(S1[\"mean\"])\n",
    "ax[1].hist(S0[\"mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring K\n",
    "\n",
    "This model infers K and uses that to predict FYA instead of relying on R,S (which are parents of G,L making those indirectly biased). First trains on all features (including R and S). Then learns K and uses that to predict F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>A</th>\n",
       "      <th>G</th>\n",
       "      <th>L</th>\n",
       "      <th>F</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(0.)</td>\n",
       "      <td>tensor(0.7251)</td>\n",
       "      <td>tensor(4.0518)</td>\n",
       "      <td>tensor(6.5633)</td>\n",
       "      <td>tensor(3.3336)</td>\n",
       "      <td>tensor(0.0160)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(0.3221)</td>\n",
       "      <td>tensor(5.4085)</td>\n",
       "      <td>tensor(6.6275)</td>\n",
       "      <td>tensor(3.7261)</td>\n",
       "      <td>tensor(0.0458)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(0.)</td>\n",
       "      <td>tensor(-0.4236)</td>\n",
       "      <td>tensor(1.8130)</td>\n",
       "      <td>tensor(5.1053)</td>\n",
       "      <td>tensor(2.1210)</td>\n",
       "      <td>tensor(0.0123)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tensor(0.)</td>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(-0.8122)</td>\n",
       "      <td>tensor(2.5937)</td>\n",
       "      <td>tensor(-0.1850)</td>\n",
       "      <td>tensor(0.3713)</td>\n",
       "      <td>tensor(0.0793)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tensor(0.)</td>\n",
       "      <td>tensor(0.)</td>\n",
       "      <td>tensor(-0.6776)</td>\n",
       "      <td>tensor(0.4421)</td>\n",
       "      <td>tensor(-0.7168)</td>\n",
       "      <td>tensor(-0.8824)</td>\n",
       "      <td>tensor(0.0156)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            R           S                A               G                L  \\\n",
       "0  tensor(1.)  tensor(0.)   tensor(0.7251)  tensor(4.0518)   tensor(6.5633)   \n",
       "1  tensor(1.)  tensor(1.)   tensor(0.3221)  tensor(5.4085)   tensor(6.6275)   \n",
       "2  tensor(1.)  tensor(0.)  tensor(-0.4236)  tensor(1.8130)   tensor(5.1053)   \n",
       "3  tensor(0.)  tensor(1.)  tensor(-0.8122)  tensor(2.5937)  tensor(-0.1850)   \n",
       "4  tensor(0.)  tensor(0.)  tensor(-0.6776)  tensor(0.4421)  tensor(-0.7168)   \n",
       "\n",
       "                 F               p  \n",
       "0   tensor(3.3336)  tensor(0.0160)  \n",
       "1   tensor(3.7261)  tensor(0.0458)  \n",
       "2   tensor(2.1210)  tensor(0.0123)  \n",
       "3   tensor(0.3713)  tensor(0.0793)  \n",
       "4  tensor(-0.8824)  tensor(0.0156)  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = pd.DataFrame(columns=['R', 'S', 'A', 'G', 'L', 'F', 'p'])\n",
    "aware_sample= []\n",
    "for i in range(1000):\n",
    "    trace = trace_handler.get_trace(exo_dist)\n",
    "    R = trace.nodes['R']['value']\n",
    "    S = trace.nodes['S']['value']\n",
    "    A = trace.nodes['A']['value']\n",
    "    G = trace.nodes['G']['value']\n",
    "    L = trace.nodes['L']['value']\n",
    "    F = trace.nodes['F']['value']\n",
    "    # get prob of each combination\n",
    "    log_prob = trace.log_prob_sum()\n",
    "    p = np.exp(log_prob)\n",
    "    samples = samples.append({'R': R, 'S': S, 'A': A, 'G': G, 'L':L, 'F': F, 'p': p}, ignore_index=True)\n",
    "    aware_sample.append(([R,S,F]))\n",
    "\n",
    "samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\somet\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 4515.5381\n",
      "[iteration 0002] loss: 4515.5381\n",
      "[iteration 0003] loss: 4515.5381\n",
      "[iteration 0004] loss: 4515.5381\n",
      "[iteration 0005] loss: 4515.5381\n",
      "[iteration 0006] loss: 4515.5381\n",
      "[iteration 0007] loss: 4515.5381\n",
      "[iteration 0008] loss: 4515.5381\n",
      "[iteration 0009] loss: 4515.5381\n",
      "[iteration 0010] loss: 4515.5381\n",
      "[iteration 0011] loss: 4515.5381\n",
      "[iteration 0012] loss: 4515.5381\n",
      "[iteration 0013] loss: 4515.5381\n",
      "[iteration 0014] loss: 4515.5381\n",
      "[iteration 0015] loss: 4515.5381\n",
      "[iteration 0016] loss: 4515.5381\n",
      "[iteration 0017] loss: 4515.5381\n",
      "[iteration 0018] loss: 4515.5381\n",
      "[iteration 0019] loss: 4515.5381\n",
      "[iteration 0020] loss: 4515.5381\n",
      "[iteration 0021] loss: 4515.5381\n",
      "[iteration 0022] loss: 4515.5381\n",
      "[iteration 0023] loss: 4515.5381\n",
      "[iteration 0024] loss: 4515.5381\n",
      "[iteration 0025] loss: 4515.5381\n",
      "[iteration 0026] loss: 4515.5381\n",
      "[iteration 0027] loss: 4515.5381\n",
      "[iteration 0028] loss: 4515.5381\n",
      "[iteration 0029] loss: 4515.5381\n",
      "[iteration 0030] loss: 4515.5381\n",
      "[iteration 0031] loss: 4515.5381\n",
      "[iteration 0032] loss: 4515.5381\n",
      "[iteration 0033] loss: 4515.5381\n",
      "[iteration 0034] loss: 4515.5381\n",
      "[iteration 0035] loss: 4515.5381\n",
      "[iteration 0036] loss: 4515.5381\n",
      "[iteration 0037] loss: 4515.5381\n",
      "[iteration 0038] loss: 4515.5381\n",
      "[iteration 0039] loss: 4515.5381\n",
      "[iteration 0040] loss: 4515.5381\n",
      "[iteration 0041] loss: 4515.5381\n",
      "[iteration 0042] loss: 4515.5381\n",
      "[iteration 0043] loss: 4515.5381\n",
      "[iteration 0044] loss: 4515.5381\n",
      "[iteration 0045] loss: 4515.5381\n",
      "[iteration 0046] loss: 4515.5381\n",
      "[iteration 0047] loss: 4515.5381\n",
      "[iteration 0048] loss: 4515.5381\n",
      "[iteration 0049] loss: 4515.5381\n",
      "[iteration 0050] loss: 4515.5381\n",
      "[iteration 0051] loss: 4515.5381\n",
      "[iteration 0052] loss: 4515.5381\n",
      "[iteration 0053] loss: 4515.5381\n",
      "[iteration 0054] loss: 4515.5381\n",
      "[iteration 0055] loss: 4515.5381\n",
      "[iteration 0056] loss: 4515.5381\n",
      "[iteration 0057] loss: 4515.5381\n",
      "[iteration 0058] loss: 4515.5381\n",
      "[iteration 0059] loss: 4515.5381\n",
      "[iteration 0060] loss: 4515.5381\n",
      "[iteration 0061] loss: 4515.5381\n",
      "[iteration 0062] loss: 4515.5381\n",
      "[iteration 0063] loss: 4515.5381\n",
      "[iteration 0064] loss: 4515.5381\n",
      "[iteration 0065] loss: 4515.5381\n",
      "[iteration 0066] loss: 4515.5381\n",
      "[iteration 0067] loss: 4515.5381\n",
      "[iteration 0068] loss: 4515.5381\n",
      "[iteration 0069] loss: 4515.5381\n",
      "[iteration 0070] loss: 4515.5381\n",
      "[iteration 0071] loss: 4515.5381\n",
      "[iteration 0072] loss: 4515.5381\n",
      "[iteration 0073] loss: 4515.5381\n",
      "[iteration 0074] loss: 4515.5381\n",
      "[iteration 0075] loss: 4515.5381\n",
      "[iteration 0076] loss: 4515.5381\n",
      "[iteration 0077] loss: 4515.5381\n",
      "[iteration 0078] loss: 4515.5381\n",
      "[iteration 0079] loss: 4515.5381\n",
      "[iteration 0080] loss: 4515.5381\n",
      "[iteration 0081] loss: 4515.5381\n",
      "[iteration 0082] loss: 4515.5381\n",
      "[iteration 0083] loss: 4515.5381\n",
      "[iteration 0084] loss: 4515.5381\n",
      "[iteration 0085] loss: 4515.5381\n",
      "[iteration 0086] loss: 4515.5381\n",
      "[iteration 0087] loss: 4515.5381\n",
      "[iteration 0088] loss: 4515.5381\n",
      "[iteration 0089] loss: 4515.5381\n",
      "[iteration 0090] loss: 4515.5381\n",
      "[iteration 0091] loss: 4515.5381\n",
      "[iteration 0092] loss: 4515.5381\n",
      "[iteration 0093] loss: 4515.5381\n",
      "[iteration 0094] loss: 4515.5381\n",
      "[iteration 0095] loss: 4515.5381\n",
      "[iteration 0096] loss: 4515.5381\n",
      "[iteration 0097] loss: 4515.5381\n",
      "[iteration 0098] loss: 4515.5381\n",
      "[iteration 0099] loss: 4515.5381\n",
      "[iteration 0100] loss: 4515.5381\n",
      "[iteration 0101] loss: 4515.5381\n",
      "[iteration 0102] loss: 4515.5381\n",
      "[iteration 0103] loss: 4515.5381\n",
      "[iteration 0104] loss: 4515.5381\n",
      "[iteration 0105] loss: 4515.5381\n",
      "[iteration 0106] loss: 4515.5381\n",
      "[iteration 0107] loss: 4515.5381\n",
      "[iteration 0108] loss: 4515.5381\n",
      "[iteration 0109] loss: 4515.5381\n",
      "[iteration 0110] loss: 4515.5381\n",
      "[iteration 0111] loss: 4515.5381\n",
      "[iteration 0112] loss: 4515.5381\n",
      "[iteration 0113] loss: 4515.5381\n",
      "[iteration 0114] loss: 4515.5381\n",
      "[iteration 0115] loss: 4515.5381\n",
      "[iteration 0116] loss: 4515.5381\n",
      "[iteration 0117] loss: 4515.5381\n",
      "[iteration 0118] loss: 4515.5381\n",
      "[iteration 0119] loss: 4515.5381\n",
      "[iteration 0120] loss: 4515.5381\n",
      "[iteration 0121] loss: 4515.5381\n",
      "[iteration 0122] loss: 4515.5381\n",
      "[iteration 0123] loss: 4515.5381\n",
      "[iteration 0124] loss: 4515.5381\n",
      "[iteration 0125] loss: 4515.5381\n",
      "[iteration 0126] loss: 4515.5381\n",
      "[iteration 0127] loss: 4515.5381\n",
      "[iteration 0128] loss: 4515.5381\n",
      "[iteration 0129] loss: 4515.5381\n",
      "[iteration 0130] loss: 4515.5381\n",
      "[iteration 0131] loss: 4515.5381\n",
      "[iteration 0132] loss: 4515.5381\n",
      "[iteration 0133] loss: 4515.5381\n",
      "[iteration 0134] loss: 4515.5381\n",
      "[iteration 0135] loss: 4515.5381\n",
      "[iteration 0136] loss: 4515.5381\n",
      "[iteration 0137] loss: 4515.5381\n",
      "[iteration 0138] loss: 4515.5381\n",
      "[iteration 0139] loss: 4515.5381\n",
      "[iteration 0140] loss: 4515.5381\n",
      "[iteration 0141] loss: 4515.5381\n",
      "[iteration 0142] loss: 4515.5381\n",
      "[iteration 0143] loss: 4515.5381\n",
      "[iteration 0144] loss: 4515.5381\n",
      "[iteration 0145] loss: 4515.5381\n",
      "[iteration 0146] loss: 4515.5381\n",
      "[iteration 0147] loss: 4515.5381\n",
      "[iteration 0148] loss: 4515.5381\n",
      "[iteration 0149] loss: 4515.5381\n",
      "[iteration 0150] loss: 4515.5381\n",
      "[iteration 0151] loss: 4515.5381\n",
      "[iteration 0152] loss: 4515.5381\n",
      "[iteration 0153] loss: 4515.5381\n",
      "[iteration 0154] loss: 4515.5381\n",
      "[iteration 0155] loss: 4515.5381\n",
      "[iteration 0156] loss: 4515.5381\n",
      "[iteration 0157] loss: 4515.5381\n",
      "[iteration 0158] loss: 4515.5381\n",
      "[iteration 0159] loss: 4515.5381\n",
      "[iteration 0160] loss: 4515.5381\n",
      "[iteration 0161] loss: 4515.5381\n",
      "[iteration 0162] loss: 4515.5381\n",
      "[iteration 0163] loss: 4515.5381\n",
      "[iteration 0164] loss: 4515.5381\n",
      "[iteration 0165] loss: 4515.5381\n",
      "[iteration 0166] loss: 4515.5381\n",
      "[iteration 0167] loss: 4515.5381\n",
      "[iteration 0168] loss: 4515.5381\n",
      "[iteration 0169] loss: 4515.5381\n",
      "[iteration 0170] loss: 4515.5381\n",
      "[iteration 0171] loss: 4515.5381\n",
      "[iteration 0172] loss: 4515.5381\n",
      "[iteration 0173] loss: 4515.5381\n",
      "[iteration 0174] loss: 4515.5381\n",
      "[iteration 0175] loss: 4515.5381\n",
      "[iteration 0176] loss: 4515.5381\n",
      "[iteration 0177] loss: 4515.5381\n",
      "[iteration 0178] loss: 4515.5381\n",
      "[iteration 0179] loss: 4515.5381\n",
      "[iteration 0180] loss: 4515.5381\n",
      "[iteration 0181] loss: 4515.5381\n",
      "[iteration 0182] loss: 4515.5381\n",
      "[iteration 0183] loss: 4515.5381\n",
      "[iteration 0184] loss: 4515.5381\n",
      "[iteration 0185] loss: 4515.5381\n",
      "[iteration 0186] loss: 4515.5381\n",
      "[iteration 0187] loss: 4515.5381\n",
      "[iteration 0188] loss: 4515.5381\n",
      "[iteration 0189] loss: 4515.5381\n",
      "[iteration 0190] loss: 4515.5381\n",
      "[iteration 0191] loss: 4515.5381\n",
      "[iteration 0192] loss: 4515.5381\n",
      "[iteration 0193] loss: 4515.5381\n",
      "[iteration 0194] loss: 4515.5381\n",
      "[iteration 0195] loss: 4515.5381\n",
      "[iteration 0196] loss: 4515.5381\n",
      "[iteration 0197] loss: 4515.5381\n",
      "[iteration 0198] loss: 4515.5381\n",
      "[iteration 0199] loss: 4515.5381\n",
      "[iteration 0200] loss: 4515.5381\n",
      "[iteration 0201] loss: 4515.5381\n",
      "[iteration 0202] loss: 4515.5381\n",
      "[iteration 0203] loss: 4515.5381\n",
      "[iteration 0204] loss: 4515.5381\n",
      "[iteration 0205] loss: 4515.5381\n",
      "[iteration 0206] loss: 4515.5381\n",
      "[iteration 0207] loss: 4515.5381\n",
      "[iteration 0208] loss: 4515.5381\n",
      "[iteration 0209] loss: 4515.5381\n",
      "[iteration 0210] loss: 4515.5381\n",
      "[iteration 0211] loss: 4515.5381\n",
      "[iteration 0212] loss: 4515.5381\n",
      "[iteration 0213] loss: 4515.5381\n",
      "[iteration 0214] loss: 4515.5381\n",
      "[iteration 0215] loss: 4515.5381\n",
      "[iteration 0216] loss: 4515.5381\n",
      "[iteration 0217] loss: 4515.5381\n",
      "[iteration 0218] loss: 4515.5381\n",
      "[iteration 0219] loss: 4515.5381\n",
      "[iteration 0220] loss: 4515.5381\n",
      "[iteration 0221] loss: 4515.5381\n",
      "[iteration 0222] loss: 4515.5381\n",
      "[iteration 0223] loss: 4515.5381\n",
      "[iteration 0224] loss: 4515.5381\n",
      "[iteration 0225] loss: 4515.5381\n",
      "[iteration 0226] loss: 4515.5381\n",
      "[iteration 0227] loss: 4515.5381\n",
      "[iteration 0228] loss: 4515.5381\n",
      "[iteration 0229] loss: 4515.5381\n",
      "[iteration 0230] loss: 4515.5381\n",
      "[iteration 0231] loss: 4515.5381\n",
      "[iteration 0232] loss: 4515.5381\n",
      "[iteration 0233] loss: 4515.5381\n",
      "[iteration 0234] loss: 4515.5381\n",
      "[iteration 0235] loss: 4515.5381\n",
      "[iteration 0236] loss: 4515.5381\n",
      "[iteration 0237] loss: 4515.5381\n",
      "[iteration 0238] loss: 4515.5381\n",
      "[iteration 0239] loss: 4515.5381\n",
      "[iteration 0240] loss: 4515.5381\n",
      "[iteration 0241] loss: 4515.5381\n",
      "[iteration 0242] loss: 4515.5381\n",
      "[iteration 0243] loss: 4515.5381\n",
      "[iteration 0244] loss: 4515.5381\n",
      "[iteration 0245] loss: 4515.5381\n",
      "[iteration 0246] loss: 4515.5381\n",
      "[iteration 0247] loss: 4515.5381\n",
      "[iteration 0248] loss: 4515.5381\n",
      "[iteration 0249] loss: 4515.5381\n",
      "[iteration 0250] loss: 4515.5381\n",
      "[iteration 0251] loss: 4515.5381\n",
      "[iteration 0252] loss: 4515.5381\n",
      "[iteration 0253] loss: 4515.5381\n",
      "[iteration 0254] loss: 4515.5381\n",
      "[iteration 0255] loss: 4515.5381\n",
      "[iteration 0256] loss: 4515.5381\n",
      "[iteration 0257] loss: 4515.5381\n",
      "[iteration 0258] loss: 4515.5381\n",
      "[iteration 0259] loss: 4515.5381\n",
      "[iteration 0260] loss: 4515.5381\n",
      "[iteration 0261] loss: 4515.5381\n",
      "[iteration 0262] loss: 4515.5381\n",
      "[iteration 0263] loss: 4515.5381\n",
      "[iteration 0264] loss: 4515.5381\n",
      "[iteration 0265] loss: 4515.5381\n",
      "[iteration 0266] loss: 4515.5381\n",
      "[iteration 0267] loss: 4515.5381\n",
      "[iteration 0268] loss: 4515.5381\n",
      "[iteration 0269] loss: 4515.5381\n",
      "[iteration 0270] loss: 4515.5381\n",
      "[iteration 0271] loss: 4515.5381\n",
      "[iteration 0272] loss: 4515.5381\n",
      "[iteration 0273] loss: 4515.5381\n",
      "[iteration 0274] loss: 4515.5381\n",
      "[iteration 0275] loss: 4515.5381\n",
      "[iteration 0276] loss: 4515.5381\n",
      "[iteration 0277] loss: 4515.5381\n",
      "[iteration 0278] loss: 4515.5381\n",
      "[iteration 0279] loss: 4515.5381\n",
      "[iteration 0280] loss: 4515.5381\n",
      "[iteration 0281] loss: 4515.5381\n",
      "[iteration 0282] loss: 4515.5381\n",
      "[iteration 0283] loss: 4515.5381\n",
      "[iteration 0284] loss: 4515.5381\n",
      "[iteration 0285] loss: 4515.5381\n",
      "[iteration 0286] loss: 4515.5381\n",
      "[iteration 0287] loss: 4515.5381\n",
      "[iteration 0288] loss: 4515.5381\n",
      "[iteration 0289] loss: 4515.5381\n",
      "[iteration 0290] loss: 4515.5381\n",
      "[iteration 0291] loss: 4515.5381\n",
      "[iteration 0292] loss: 4515.5381\n",
      "[iteration 0293] loss: 4515.5381\n",
      "[iteration 0294] loss: 4515.5381\n",
      "[iteration 0295] loss: 4515.5381\n",
      "[iteration 0296] loss: 4515.5381\n",
      "[iteration 0297] loss: 4515.5381\n",
      "[iteration 0298] loss: 4515.5381\n",
      "[iteration 0299] loss: 4515.5381\n",
      "[iteration 0300] loss: 4515.5381\n",
      "[iteration 0301] loss: 4515.5381\n",
      "[iteration 0302] loss: 4515.5381\n",
      "[iteration 0303] loss: 4515.5381\n",
      "[iteration 0304] loss: 4515.5381\n",
      "[iteration 0305] loss: 4515.5381\n",
      "[iteration 0306] loss: 4515.5381\n",
      "[iteration 0307] loss: 4515.5381\n",
      "[iteration 0308] loss: 4515.5381\n",
      "[iteration 0309] loss: 4515.5381\n",
      "[iteration 0310] loss: 4515.5381\n",
      "[iteration 0311] loss: 4515.5381\n",
      "[iteration 0312] loss: 4515.5381\n",
      "[iteration 0313] loss: 4515.5381\n",
      "[iteration 0314] loss: 4515.5381\n",
      "[iteration 0315] loss: 4515.5381\n",
      "[iteration 0316] loss: 4515.5381\n",
      "[iteration 0317] loss: 4515.5381\n",
      "[iteration 0318] loss: 4515.5381\n",
      "[iteration 0319] loss: 4515.5381\n",
      "[iteration 0320] loss: 4515.5381\n",
      "[iteration 0321] loss: 4515.5381\n",
      "[iteration 0322] loss: 4515.5381\n",
      "[iteration 0323] loss: 4515.5381\n",
      "[iteration 0324] loss: 4515.5381\n",
      "[iteration 0325] loss: 4515.5381\n",
      "[iteration 0326] loss: 4515.5381\n",
      "[iteration 0327] loss: 4515.5381\n",
      "[iteration 0328] loss: 4515.5381\n",
      "[iteration 0329] loss: 4515.5381\n",
      "[iteration 0330] loss: 4515.5381\n",
      "[iteration 0331] loss: 4515.5381\n",
      "[iteration 0332] loss: 4515.5381\n",
      "[iteration 0333] loss: 4515.5381\n",
      "[iteration 0334] loss: 4515.5381\n",
      "[iteration 0335] loss: 4515.5381\n",
      "[iteration 0336] loss: 4515.5381\n",
      "[iteration 0337] loss: 4515.5381\n",
      "[iteration 0338] loss: 4515.5381\n",
      "[iteration 0339] loss: 4515.5381\n",
      "[iteration 0340] loss: 4515.5381\n",
      "[iteration 0341] loss: 4515.5381\n",
      "[iteration 0342] loss: 4515.5381\n",
      "[iteration 0343] loss: 4515.5381\n",
      "[iteration 0344] loss: 4515.5381\n",
      "[iteration 0345] loss: 4515.5381\n",
      "[iteration 0346] loss: 4515.5381\n",
      "[iteration 0347] loss: 4515.5381\n",
      "[iteration 0348] loss: 4515.5381\n",
      "[iteration 0349] loss: 4515.5381\n",
      "[iteration 0350] loss: 4515.5381\n",
      "[iteration 0351] loss: 4515.5381\n",
      "[iteration 0352] loss: 4515.5381\n",
      "[iteration 0353] loss: 4515.5381\n",
      "[iteration 0354] loss: 4515.5381\n",
      "[iteration 0355] loss: 4515.5381\n",
      "[iteration 0356] loss: 4515.5381\n",
      "[iteration 0357] loss: 4515.5381\n",
      "[iteration 0358] loss: 4515.5381\n",
      "[iteration 0359] loss: 4515.5381\n",
      "[iteration 0360] loss: 4515.5381\n",
      "[iteration 0361] loss: 4515.5381\n",
      "[iteration 0362] loss: 4515.5381\n",
      "[iteration 0363] loss: 4515.5381\n",
      "[iteration 0364] loss: 4515.5381\n",
      "[iteration 0365] loss: 4515.5381\n",
      "[iteration 0366] loss: 4515.5381\n",
      "[iteration 0367] loss: 4515.5381\n",
      "[iteration 0368] loss: 4515.5381\n",
      "[iteration 0369] loss: 4515.5381\n",
      "[iteration 0370] loss: 4515.5381\n",
      "[iteration 0371] loss: 4515.5381\n",
      "[iteration 0372] loss: 4515.5381\n",
      "[iteration 0373] loss: 4515.5381\n",
      "[iteration 0374] loss: 4515.5381\n",
      "[iteration 0375] loss: 4515.5381\n",
      "[iteration 0376] loss: 4515.5381\n",
      "[iteration 0377] loss: 4515.5381\n",
      "[iteration 0378] loss: 4515.5381\n",
      "[iteration 0379] loss: 4515.5381\n",
      "[iteration 0380] loss: 4515.5381\n",
      "[iteration 0381] loss: 4515.5381\n",
      "[iteration 0382] loss: 4515.5381\n",
      "[iteration 0383] loss: 4515.5381\n",
      "[iteration 0384] loss: 4515.5381\n",
      "[iteration 0385] loss: 4515.5381\n",
      "[iteration 0386] loss: 4515.5381\n",
      "[iteration 0387] loss: 4515.5381\n",
      "[iteration 0388] loss: 4515.5381\n",
      "[iteration 0389] loss: 4515.5381\n",
      "[iteration 0390] loss: 4515.5381\n",
      "[iteration 0391] loss: 4515.5381\n",
      "[iteration 0392] loss: 4515.5381\n",
      "[iteration 0393] loss: 4515.5381\n",
      "[iteration 0394] loss: 4515.5381\n",
      "[iteration 0395] loss: 4515.5381\n",
      "[iteration 0396] loss: 4515.5381\n",
      "[iteration 0397] loss: 4515.5381\n",
      "[iteration 0398] loss: 4515.5381\n",
      "[iteration 0399] loss: 4515.5381\n",
      "[iteration 0400] loss: 4515.5381\n",
      "[iteration 0401] loss: 4515.5381\n",
      "[iteration 0402] loss: 4515.5381\n",
      "[iteration 0403] loss: 4515.5381\n",
      "[iteration 0404] loss: 4515.5381\n",
      "[iteration 0405] loss: 4515.5381\n",
      "[iteration 0406] loss: 4515.5381\n",
      "[iteration 0407] loss: 4515.5381\n",
      "[iteration 0408] loss: 4515.5381\n",
      "[iteration 0409] loss: 4515.5381\n",
      "[iteration 0410] loss: 4515.5381\n",
      "[iteration 0411] loss: 4515.5381\n",
      "[iteration 0412] loss: 4515.5381\n",
      "[iteration 0413] loss: 4515.5381\n",
      "[iteration 0414] loss: 4515.5381\n",
      "[iteration 0415] loss: 4515.5381\n",
      "[iteration 0416] loss: 4515.5381\n",
      "[iteration 0417] loss: 4515.5381\n",
      "[iteration 0418] loss: 4515.5381\n",
      "[iteration 0419] loss: 4515.5381\n",
      "[iteration 0420] loss: 4515.5381\n",
      "[iteration 0421] loss: 4515.5381\n",
      "[iteration 0422] loss: 4515.5381\n",
      "[iteration 0423] loss: 4515.5381\n",
      "[iteration 0424] loss: 4515.5381\n",
      "[iteration 0425] loss: 4515.5381\n",
      "[iteration 0426] loss: 4515.5381\n",
      "[iteration 0427] loss: 4515.5381\n",
      "[iteration 0428] loss: 4515.5381\n",
      "[iteration 0429] loss: 4515.5381\n",
      "[iteration 0430] loss: 4515.5381\n",
      "[iteration 0431] loss: 4515.5381\n",
      "[iteration 0432] loss: 4515.5381\n",
      "[iteration 0433] loss: 4515.5381\n",
      "[iteration 0434] loss: 4515.5381\n",
      "[iteration 0435] loss: 4515.5381\n",
      "[iteration 0436] loss: 4515.5381\n",
      "[iteration 0437] loss: 4515.5381\n",
      "[iteration 0438] loss: 4515.5381\n",
      "[iteration 0439] loss: 4515.5381\n",
      "[iteration 0440] loss: 4515.5381\n",
      "[iteration 0441] loss: 4515.5381\n",
      "[iteration 0442] loss: 4515.5381\n",
      "[iteration 0443] loss: 4515.5381\n",
      "[iteration 0444] loss: 4515.5381\n",
      "[iteration 0445] loss: 4515.5381\n",
      "[iteration 0446] loss: 4515.5381\n",
      "[iteration 0447] loss: 4515.5381\n",
      "[iteration 0448] loss: 4515.5381\n",
      "[iteration 0449] loss: 4515.5381\n",
      "[iteration 0450] loss: 4515.5381\n",
      "[iteration 0451] loss: 4515.5381\n",
      "[iteration 0452] loss: 4515.5381\n",
      "[iteration 0453] loss: 4515.5381\n",
      "[iteration 0454] loss: 4515.5381\n",
      "[iteration 0455] loss: 4515.5381\n",
      "[iteration 0456] loss: 4515.5381\n",
      "[iteration 0457] loss: 4515.5381\n",
      "[iteration 0458] loss: 4515.5381\n",
      "[iteration 0459] loss: 4515.5381\n",
      "[iteration 0460] loss: 4515.5381\n",
      "[iteration 0461] loss: 4515.5381\n",
      "[iteration 0462] loss: 4515.5381\n",
      "[iteration 0463] loss: 4515.5381\n",
      "[iteration 0464] loss: 4515.5381\n",
      "[iteration 0465] loss: 4515.5381\n",
      "[iteration 0466] loss: 4515.5381\n",
      "[iteration 0467] loss: 4515.5381\n",
      "[iteration 0468] loss: 4515.5381\n",
      "[iteration 0469] loss: 4515.5381\n",
      "[iteration 0470] loss: 4515.5381\n",
      "[iteration 0471] loss: 4515.5381\n",
      "[iteration 0472] loss: 4515.5381\n",
      "[iteration 0473] loss: 4515.5381\n",
      "[iteration 0474] loss: 4515.5381\n",
      "[iteration 0475] loss: 4515.5381\n",
      "[iteration 0476] loss: 4515.5381\n",
      "[iteration 0477] loss: 4515.5381\n",
      "[iteration 0478] loss: 4515.5381\n",
      "[iteration 0479] loss: 4515.5381\n",
      "[iteration 0480] loss: 4515.5381\n",
      "[iteration 0481] loss: 4515.5381\n",
      "[iteration 0482] loss: 4515.5381\n",
      "[iteration 0483] loss: 4515.5381\n",
      "[iteration 0484] loss: 4515.5381\n",
      "[iteration 0485] loss: 4515.5381\n",
      "[iteration 0486] loss: 4515.5381\n",
      "[iteration 0487] loss: 4515.5381\n",
      "[iteration 0488] loss: 4515.5381\n",
      "[iteration 0489] loss: 4515.5381\n",
      "[iteration 0490] loss: 4515.5381\n",
      "[iteration 0491] loss: 4515.5381\n",
      "[iteration 0492] loss: 4515.5381\n",
      "[iteration 0493] loss: 4515.5381\n",
      "[iteration 0494] loss: 4515.5381\n",
      "[iteration 0495] loss: 4515.5381\n",
      "[iteration 0496] loss: 4515.5381\n",
      "[iteration 0497] loss: 4515.5381\n",
      "[iteration 0498] loss: 4515.5381\n",
      "[iteration 0499] loss: 4515.5381\n",
      "[iteration 0500] loss: 4515.5381\n",
      "Learned parameters:\n",
      "weight [[0.2779894]]\n",
      "bias [0.5043056]\n"
     ]
    }
   ],
   "source": [
    "#Data to regress\n",
    "aware_sample = torch.tensor(aware_sample)\n",
    "x_data, y_data = aware_sample[:, :-1], aware_sample[:, -1]\n",
    "a_data = torch.tensor(samples.A)\n",
    "\n",
    "# Regression model\n",
    "# 2 = in features, 1=out feature\n",
    "G_linear_reg_model = PyroModule[nn.Linear](2, 1) # R and S\n",
    "L_linear_reg_model = PyroModule[nn.Linear](2, 1) # R and S\n",
    "A_linear_reg_model = PyroModule[nn.Linear](2, 1) # G and L\n",
    "linear_reg_model = PyroModule[nn.Linear](1,1) # just A\n",
    "\n",
    "# Define loss and optimize\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "G_optim = torch.optim.Adam(G_linear_reg_model.parameters(), lr=0.05)\n",
    "L_optim = torch.optim.Adam(L_linear_reg_model.parameters(), lr=0.05)\n",
    "A_optim = torch.optim.Adam(A_linear_reg_model.parameters(), lr=0.05)\n",
    "num_iterations = 500 if not smoke_test else 2\n",
    "\n",
    "def train():\n",
    "    # learn G/L\n",
    "    for j in range(num_iterations):\n",
    "        g_pred = G_linear_reg_model(x_data).squeeze(-1)\n",
    "        g_loss = loss_fn(g_pred, torch.tensor(samples.G))\n",
    "        G_optim.zero_grad()\n",
    "        g_loss.backward()\n",
    "        G_optim.step\n",
    "\n",
    "        l_pred = L_linear_reg_model(x_data).squeeze(-1)\n",
    "        l_loss = loss_fn(l_pred, torch.tensor(samples.L))\n",
    "        L_optim.zero_grad()\n",
    "        l_loss.backward()\n",
    "        L_optim.step\n",
    "        \n",
    "    # use learned G/L predictions\n",
    "    bundled_pred=torch.tensor([[g_pred[i], l_pred[i]] for i in range(1000)])\n",
    "    \n",
    "    # learn A\n",
    "    for j in range(num_iterations):\n",
    "        A_pred = A_linear_reg_model(bundled_pred).squeeze(-1)\n",
    "        a_loss = loss_fn(A_pred, torch.tensor(samples.A))\n",
    "        A_optim.zero_grad()\n",
    "        a_loss.backward()\n",
    "        A_optim.step\n",
    "    A_pred = torch.tensor([[A_pred[i]] for i in range(1000)])\n",
    "    \n",
    "    # calculate FYA\n",
    "    for j in range(num_iterations):\n",
    "        y_pred = linear_reg_model(A_pred).squeeze(-1)\n",
    "        # calculate the mse loss\n",
    "        loss = loss_fn(y_pred, y_data)\n",
    "        # initialize gradients to zero\n",
    "        optim.zero_grad()\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # take a gradient step\n",
    "        optim.step()\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss.item()))\n",
    "    return A_pred\n",
    "    \n",
    "\n",
    "\n",
    "# Inspect learned parameters\n",
    "A_pred = train()\n",
    "print(\"Learned parameters:\")\n",
    "for name, param in linear_reg_model.named_parameters():\n",
    "    print(name, param.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([461.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 193.]),\n",
       " array([0.4549464 , 0.45644417, 0.45794195, 0.45943975, 0.46093753,\n",
       "        0.4624353 , 0.46393308, 0.46543086, 0.46692866, 0.46842644,\n",
       "        0.4699242 ], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit = samples.copy()\n",
    "fit[\"mean\"] = linear_reg_model(A_pred).detach().numpy()\n",
    "\n",
    "S1 = fit[fit[\"S\"] == 1]\n",
    "S0 = fit[fit[\"S\"] == 0]\n",
    "R1 = fit[fit[\"R\"] == 1]\n",
    "R0 = fit[fit[\"R\"] == 0]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n",
    "ax[0].hist(R1[\"mean\"])\n",
    "ax[0].hist(R0[\"mean\"])\n",
    "ax[1].hist(S1[\"mean\"])\n",
    "ax[1].hist(S0[\"mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
