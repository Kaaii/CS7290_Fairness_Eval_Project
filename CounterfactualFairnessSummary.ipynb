{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Counterfactual Fairness Replication** \n",
    "### By Potluri, Kai, Mingfei, and Ollie\n",
    "\n",
    "There are many machine learning algorithms that can and do automate decisions for complex issues such as receiving a loan, insurance pricing, predictive policing, prison sentences, etc. This  paper, by Kusner, Loftus, Russell, and Silva, seeks to develop a way to address whether a prediction is fair using tools from causal inference. Through causal modeling we can see how and where this unfairness is happening. Because algorithms use observed data to make predictions, data that may contain historical bias, (ex. racially biased police stop and frisk policies sexist/racist/ageist hiring policies) they can often result in unfair policy decisions. \n",
    "\n",
    "To understand how to evaluate fairness we must define what it means. Below are a definitions presented by the authors: \n",
    "\n",
    "- **Fairness through unawareness** – an algorithm does not use any protected attributes. If we have observations of a person’s race or sex we just ignore that information\n",
    "\n",
    "- **Individual Fairness** – an algorithm gives similar predictions to similar individuals\n",
    "\n",
    "- **Demographic Parity** – an algorithm gives the same prediction no matter what the value of the given predictor is\n",
    "\n",
    "- **Equality of Opportunity** – an algorithm gives the same prediction given the outcome regardless of the protected attribute\n",
    "\n",
    "- **Counterfactual Fairness** – Had any individual been of a different race, sex, etc. the prediction would not change\n",
    "\n",
    "The authors believe that counterfactual fairness is the correct definition to use because we can make comparisons on an individual level. We are asking what would have happened if this same person was of a different race or sex.\n",
    "\n",
    "Below is a simple example from the paper to show how counterfactual awareness works. We are looking to predict the accident rate of an individual given we have observations on some protected attribute and the color of their car. See below for the complete model.\n",
    "\n",
    "- **A Protected Attribute** - any attribute that should not be discriminated \n",
    "- **X Observable attribute** – Red Car\n",
    "- **Y outcome of interest** – Accident Rate\n",
    "- **U latent variable** – Aggressive Driving\n",
    "\n",
    "\n",
    "    \n",
    "![image alt ><](project_DAG_2.png)\n",
    "    \n",
    "\n",
    "\n",
    "In this example, some group A is more likely than other groups to drive a red car but are not more likely to get into an accident. However, people who are more likely to be aggressive drivers like to drive red cars as well. We are explicitly modeling a discriminatory effect as a causal effect. If we were to predict who would have a high accident rate Y by using just X we would have a counterfactually unfair prediction because we know that individuals of the protected attribute like to drive red cars more than other groups even though they don’t get into more accidents.\n",
    "\n",
    "One of the main implications of counterfactual fairness is that a prediction will be counterfactually fair if it is a function of the non-descendants of A. In the example above we could not use the observable attribute red car to make a counterfactually fair prediction as any change in A would cause a change in the observed variable, red car. Instead we would want to try to infer the latent variable U.\n",
    "\n",
    "This is not an if and only if statement however, it is possible for a prediction to be counterfactually fair if it is a function of a descendent of the protected attribute A. However, this is only true though if the dependence of the prediction on the protected attribute disappears in the function.\n",
    "\n",
    "Another benefit of using causal inference to evaluate fairness is the ability to deal with historical bias. An example of this given in the paper is of whether a person defaults on a loan. A protected attribute may cause a person to default on a loan, but only through some past discrimination. The protected attribute for example may be mediated by employment which in turn is caused by the latent variable, prejudice. Prejudice could mean that the hiring process was unfair towards certain groups. This causes a person to be less likely to be employed and in turn increases the likelihood of default. Hence Y, a descendent of A, has hsitorical discrimination and should not be used in the prediction of $\\hat{Y}$.\n",
    "\n",
    "We want to use variables that are not caused by A, the protected attribute, but are predictive of Y. Below is the algorithm given in the text.\n",
    "\n",
    "### Fair Learning Algorithm\n",
    "\n",
    "![](FairLearning Algorithm.png)\n",
    "\n",
    "Essentially what the algorithm is saying is, given the observed values of A and X, extract the values of the latent variable U and then use those values in the predictive model. \n",
    "\n",
    "There are three different levels of assumptions that make counterfactual fairness possible. The higher levels (3 being the highest) require stronger assumptions\n",
    "\n",
    "- Level 1 – Determine $\\hat{Y}$ from observable non-descendents of A\n",
    "- Level 2 – Determine $\\hat{Y}$ through some non-deterministic latent variable \n",
    "- Level 3 – Determine $\\hat{Y}$ through some deterministic latent variables (not discussed in great detail) \n",
    "\n",
    "The authors recommend always modeling the causal structure to reach the goal of counterfactual fairness. They state, “we are essentially learning a projection of T into the space of fair decisions, removing historical biases as a by-product ”. (p. 5) Modeling in this way does not always give the most accurate predictions, however this is not the goal of the procedure. The goal is to accurately model the real world including social bias's that may arise and to use causal modeling tools to address algorithmic unfariness. The following is another example of this in practice.\n",
    "\n",
    "## **Law School Success** \n",
    "\n",
    "![](project_dag.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
