{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import Importance, EmpiricalMarginal\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from statistics import mean, stdev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible steps in final report\n",
    "\n",
    "1) Define the example problem (What will the variables be?)\n",
    "\n",
    "2) Define the DAG(s). \n",
    "\n",
    "3) Code the model(s) with probabilities. \n",
    "\n",
    "4) Show result of an unfair classifier (gives different predictions when the protected attribute A is changed) and a fair classifier (predictions do not change when A is changed).\n",
    "\n",
    "5) Conclusion\n",
    "\n",
    "\n",
    "References: \n",
    "\n",
    "The paper: https://arxiv.org/pdf/1703.06856.pdf\n",
    "\n",
    "Video on lecture by M. Kusner (author of the paper above): https://www.youtube.com/watch?v=ZfuOw02U7hs&ab_channel=TheAlanTuringInstitute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some definitions\n",
    "\n",
    "A metric and algorithm to show if a given model/algorithm is fair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the video lecture model where we try to calculate unobserved confounder 'A' (law ability) for each instance to see if classifier is fair\n",
    "\n",
    "We have the following exogenous variables that are protected:\n",
    "* R = race (binary for simplicity: 0 or 1) -- this can refer to maybe \"Not Race X vs. Race X\"\n",
    "* S = sex (binary: 0 or 1) -- 0 for female, 1 for male\n",
    "\n",
    "and the following observed (endogenous) variables:\n",
    "* G = GPA\n",
    "* L = LSTAT score\n",
    "\n",
    "which are used by the naive policy (from original nb) to predict the first year average grade (indicator of law school success here). \n",
    "\n",
    "* F = First year average grade\n",
    "\n",
    "There is also an unobserved exogenous variable that we'd like to do inference on:\n",
    "* A = law ability\n",
    "\n",
    "\n",
    "The DAG however shows R and S are parents of G and L in a causal DAG, so by using G and L only without taking into account R and S, you have an unaware policy that is unfair (NOTE: show this by doing a counterfactual with the unaware model where you do(R=r) or do(S=s) and see a difference in F). The fair policy takes into account R and S and uses R, S, G, and L to infer what 'A' (law ability) is and use that to predict F. The paper shows a higher accuracy when doing so when using the causal DAG where A is non-deterministic of G, L, and F (vs. the other causal DAG with A1 -> G, A2 -> L, A3 -> F). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Model \n",
    "\n",
    "The model is defined by: https://nbviewer.jupyter.org/github/apedawi-cs/Causal-inference-discussion/blob/master/law_school.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return (x - np.mean(x)) / np.std(x)\n",
    "\n",
    "def minmax_normalizer(df):\n",
    "    return (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "def simulate_exogenous_vars(nb_obs, R_pct=0.5, S_pct=0.5):\n",
    "    assert isinstance(R_pct, float) and (0 <= R_pct <= 1)\n",
    "    assert isinstance(S_pct, float) and (0 <= S_pct <= 1)\n",
    "    R = 1. * (np.random.uniform(low=0, high=1, size=[nb_obs, 1]) < R_pct)\n",
    "    S = 1. * (np.random.uniform(low=0, high=1, size=[nb_obs, 1]) < S_pct)\n",
    "    A = np.random.randn(nb_obs, 1)\n",
    "    return R, S, A\n",
    "\n",
    "def simulate_endogenous_vars(A, R, S):\n",
    "    assert A.shape == R.shape == S.shape\n",
    "    nb_obs = A.shape[0]\n",
    "    G = A + 2.1 * R + 3.3 * S + 0.5 * np.random.randn(nb_obs, 1)\n",
    "    L = A + 5.8 * R + 0.7 * S + 0.1 * np.random.randn(nb_obs, 1)\n",
    "    F = A + 2.3 * R + 1.0 * S + 0.3 * np.random.randn(nb_obs, 1)\n",
    "    return G, L, F\n",
    "\n",
    "r,s,a = simulate_exogenous_vars(1000, 0.65, 0.35)\n",
    "g,l,f = simulate_endogenous_vars(a,r,s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model in Pyro\n",
    "\n",
    "counterfactual tutorial: https://github.com/robertness/causalML/blob/master/tutorials/3-counterfactual/counterfactuals_in_pyro.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>A</th>\n",
       "      <th>G</th>\n",
       "      <th>L</th>\n",
       "      <th>F</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(0.)</td>\n",
       "      <td>tensor(1.2884)</td>\n",
       "      <td>tensor(3.8376)</td>\n",
       "      <td>tensor(6.8743)</td>\n",
       "      <td>tensor(3.4224)</td>\n",
       "      <td>tensor(0.0194)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(0.)</td>\n",
       "      <td>tensor(0.5731)</td>\n",
       "      <td>tensor(2.9436)</td>\n",
       "      <td>tensor(6.3339)</td>\n",
       "      <td>tensor(2.5603)</td>\n",
       "      <td>tensor(0.3029)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(0.)</td>\n",
       "      <td>tensor(-0.5682)</td>\n",
       "      <td>tensor(0.8972)</td>\n",
       "      <td>tensor(5.2896)</td>\n",
       "      <td>tensor(1.2763)</td>\n",
       "      <td>tensor(0.0781)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(0.6181)</td>\n",
       "      <td>tensor(5.8989)</td>\n",
       "      <td>tensor(7.1206)</td>\n",
       "      <td>tensor(3.7707)</td>\n",
       "      <td>tensor(0.2944)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(-0.4774)</td>\n",
       "      <td>tensor(4.0657)</td>\n",
       "      <td>tensor(5.9872)</td>\n",
       "      <td>tensor(2.5261)</td>\n",
       "      <td>tensor(0.0490)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            R           S                A               G               L  \\\n",
       "0  tensor(1.)  tensor(0.)   tensor(1.2884)  tensor(3.8376)  tensor(6.8743)   \n",
       "1  tensor(1.)  tensor(0.)   tensor(0.5731)  tensor(2.9436)  tensor(6.3339)   \n",
       "2  tensor(1.)  tensor(0.)  tensor(-0.5682)  tensor(0.8972)  tensor(5.2896)   \n",
       "3  tensor(1.)  tensor(1.)   tensor(0.6181)  tensor(5.8989)  tensor(7.1206)   \n",
       "4  tensor(1.)  tensor(1.)  tensor(-0.4774)  tensor(4.0657)  tensor(5.9872)   \n",
       "\n",
       "                F               p  \n",
       "0  tensor(3.4224)  tensor(0.0194)  \n",
       "1  tensor(2.5603)  tensor(0.3029)  \n",
       "2  tensor(1.2763)  tensor(0.0781)  \n",
       "3  tensor(3.7707)  tensor(0.2944)  \n",
       "4  tensor(2.5261)  tensor(0.0490)  "
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.set_rng_seed(2)\n",
    "\n",
    "exo_dist = {\n",
    "    'Nr': dist.Bernoulli(torch.tensor(0.7)),\n",
    "    'Ns': dist.Bernoulli(torch.tensor(0.35)),\n",
    "    'Na': dist.Normal(torch.tensor(0.), torch.tensor(1.))\n",
    "}\n",
    "\n",
    "def model(exo_dist):\n",
    "    # sample from bernoulli 0 or 1, 0 at 70% freq (made up)\n",
    "    R = pyro.sample(\"R\", exo_dist['Nr'])\n",
    "    S = pyro.sample(\"S\", exo_dist['Ns'])\n",
    "    \n",
    "    # random gaussian dist for ability \n",
    "    A = pyro.sample(\"A\", exo_dist['Na'])\n",
    "    \n",
    "    \n",
    "    G = pyro.sample(\"G\", dist.Normal(A + 2.1 * R + 3.3 * S, 0.5))\n",
    "    \n",
    "    L = pyro.sample(\"L\", dist.Normal(A + 5.8*R + 0.7*S, 0.1))\n",
    "    \n",
    "    F = pyro.sample(\"F\", dist.Normal(A + 2.3*R + 1.*S, 0.3))\n",
    "\n",
    "trace_handler = pyro.poutine.trace(model)\n",
    "samples = pd.DataFrame(columns=['R', 'S', 'A', 'G', 'L', 'F', 'p'])\n",
    "for i in range(1000):\n",
    "    trace = trace_handler.get_trace(exo_dist)\n",
    "    R = trace.nodes['R']['value']\n",
    "    S = trace.nodes['S']['value']\n",
    "    A = trace.nodes['A']['value']\n",
    "    G = trace.nodes['G']['value']\n",
    "    L = trace.nodes['L']['value']\n",
    "    F = trace.nodes['F']['value']\n",
    "    # get prob of each combination\n",
    "    log_prob = trace.log_prob_sum()\n",
    "    p = np.exp(log_prob)\n",
    "    samples = samples.append({'R': R, 'S': S, 'A': A, 'G': G, 'L':L, 'F': F, 'p': p}, ignore_index=True)\n",
    "samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can compare between using the original notebook's sample results vs. pyro version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(6.7913), tensor(-3.2315))"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(samples['F']), min(samples['F'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.29013306]), array([-2.86325914]))"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(f), min(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5301)"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(samples['G'])/NUM_SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G\n",
      "tensor(2.5301)\n",
      "L\n",
      "tensor(4.2523)\n",
      "F\n",
      "tensor(1.9122)\n"
     ]
    }
   ],
   "source": [
    "# some means\n",
    "print(\"G\")\n",
    "print(sum(samples['G'])/NUM_SAMPLES)\n",
    "\n",
    "print(\"L\")\n",
    "print(sum(samples['L'])/NUM_SAMPLES)\n",
    "\n",
    "print(\"F\")\n",
    "print(sum(samples['F'])/NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditioned Model\n",
    "\n",
    "Condition on a person being G=0, L=0, F=0, infer the exogenous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of R:  1.0\n",
      "Mean of S:  1.0\n",
      "Mean of A:  -2.177851146221161 Stdev:  0.06189118141212581\n"
     ]
    }
   ],
   "source": [
    "NUM_SAMPLES=1000\n",
    "\n",
    "# exo distribution to use\n",
    "exo_dist = {\n",
    "    'Nr': dist.Bernoulli(torch.tensor(0.7)),\n",
    "    'Ns': dist.Bernoulli(torch.tensor(0.35)),\n",
    "    'Na': dist.Normal(torch.tensor(0.), torch.tensor(1.))\n",
    "}\n",
    "\n",
    "conditioned = pyro.condition(\n",
    "    model,\n",
    "    {'G': torch.tensor(2.5), 'L': torch.tensor(4.2), 'F': torch.tensor(1.9)}\n",
    ")\n",
    "post = Importance(conditioned, num_samples=NUM_SAMPLES).run(exo_dist)\n",
    "R_marginal = EmpiricalMarginal(post, \"R\")\n",
    "R_samples = [R_marginal().item() for _ in range(NUM_SAMPLES)]\n",
    "R_unique, R_counts = np.unique(R_samples, return_counts=True)\n",
    "print(\"Mean of R: \", mean(R_samples))\n",
    "\n",
    "S_marginal = EmpiricalMarginal(post, \"S\")\n",
    "S_samples = [S_marginal().item() for _ in range(NUM_SAMPLES)]\n",
    "S_unique, S_counts = np.unique(S_samples, return_counts=True)\n",
    "print(\"Mean of S: \", mean(S_samples))\n",
    "\n",
    "A_marginal = EmpiricalMarginal(post, \"A\")\n",
    "A_samples = [A_marginal().item() for _ in range(NUM_SAMPLES)]\n",
    "A_unique, A_counts = np.unique(A_samples, return_counts=True)\n",
    "print(\"Mean of A: \", mean(A_samples), \"Stdev: \", stdev(A_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Model\n",
    "\n",
    "What if that same person was Male (S=1) instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagate posterior of exogenous variables:\n",
    "updated_exo_dist = {\n",
    "    'Nr': dist.Bernoulli(torch.tensor(mean(R_samples))),\n",
    "    'Ns': dist.Bernoulli(torch.tensor(mean(S_samples))),\n",
    "    'Na': dist.Normal(torch.tensor(mean(A_samples)), stdev(A_samples))\n",
    "}\n",
    "\n",
    "do_model = pyro.do(conditioned, data={\"S\": torch.tensor(1.)})\n",
    "# use pyro.infer.Importance\n",
    "do_post = pyro.infer.Importance(do_model, num_samples=NUM_SAMPLES).run(updated_exo_dist)\n",
    "do_marginal = pyro.infer.EmpiricalMarginal(do_post, \"A\")\n",
    "do_samples = [do_marginal().item() for _ in range(NUM_SAMPLES)]\n",
    "do_unique, do_counts = np.unique(do_samples, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of A:  -2.197156010866165\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean of A: \", mean(do_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019304864645004027\n"
     ]
    }
   ],
   "source": [
    "# difference between means:\n",
    "print(mean(A_samples) - mean(do_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Policy explanation\n",
    "\n",
    "Naive policy only takes into account G and L. It uses a combination of these 2 values and selects the top `nb_seats` instances where G+L is highest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_policy(G,L,nb_seats=200):\n",
    "    \"\"\"Returns array of binary predictions for law school acceptance based on naive policy\n",
    "    that only takes into account G and L.\"\"\"\n",
    "    assert G.shape == L.shape == F.shape\n",
    "    nb_obs = G.shape[0]\n",
    "    # P is predictions\n",
    "    # set P to be same size as number of observations\n",
    "    # set all to False for now \n",
    "    P = np.zeros([nb_obs,1]).astype(bool) \n",
    "    # if nb_seats is less than nb_obs, just set nb_seats to be nb_obs (everyone is accepted)\n",
    "    if nb_seats > nb_obs:\n",
    "        nb_seats = nb_obs \n",
    "    # get the top nb_seats values \n",
    "    ind = (normalize(G)+normalize(L)).argsort(axis=0)[-nb_seats:][::-1]\n",
    "    print(ind.shape)\n",
    "    # set the top nb_seats to be True by index\n",
    "    P[ind] = True\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code from Original Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the policies\n",
    "#For better understanding check this repo\n",
    "# https://github.com/mkusner/counterfactual-fairness/blob/master/law_school_classifiers.R\n",
    "# Original:\n",
    "# https://github.com/apedawi-cs/Causal-inference-discussion/blob/master/law_school.ipynb\n",
    "# paper:\n",
    "#https://arxiv.org/abs/1703.06856\n",
    "# To understand the list index thing:\n",
    "# https://www.kite.com/python/answers/how-to-use-numpy-argsort-in-descending-order-in-python\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NaivePolicy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def evaluate(self, G, L, nb_seats=None):\n",
    "        assert G.shape == L.shape\n",
    "        nb_obs = G.shape[0]\n",
    "        if nb_seats is None:\n",
    "            nb_seats = nb_obs\n",
    "        else:\n",
    "            assert isinstance(nb_seats, int) and (nb_seats > 0)\n",
    "            nb_seats = min(nb_obs, nb_seats)\n",
    "        ind = (normalize(G) + normalize(L)).argsort(axis=0)[-nb_seats:][::-1]\n",
    "        P = np.zeros([nb_obs, 1]).astype(bool)\n",
    "        P[ind] = True\n",
    "        return P\n",
    "\n",
    "class UnawarePolicy:\n",
    "    #Unaware = ZFYA ~ LSAT + UGPA \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def train(self, G, L, F):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(np.hstack([G, L]), F, test_size=0.33)\n",
    "        self.F_reg = LinearRegression().fit(X_train,y_train)\n",
    "        y_pred    = self.F_reg.predict(X_test)\n",
    "        #print(np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\n",
    "        \n",
    "        \n",
    "    def evaluate(self, G, L, nb_seats=None):\n",
    "        assert G.shape == L.shape\n",
    "        nb_obs = G.shape[0]\n",
    "        if nb_seats is None:\n",
    "            nb_seats = nb_obs\n",
    "        else:\n",
    "            assert isinstance(nb_seats, int) and (nb_seats > 0)\n",
    "            nb_seats = min(nb_obs, nb_seats)\n",
    "        F_hat = self.F_reg.predict(np.hstack([G, L]))\n",
    "        ind = F_hat.argsort(axis=0)[-nb_seats:][::-1] #get the indexes in sorted ascending order using ranking\n",
    "        P = np.zeros([nb_obs, 1]).astype(bool)\n",
    "        P[ind] = True\n",
    "        return P\n",
    "\n",
    "class FairPolicy:\n",
    "    #model-ugpa =\n",
    "    #model-lsat =\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def train(self, R, S, G, L):\n",
    "        self.G_reg = LinearRegression().fit(np.hstack([R, S]), G)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(np.hstack([R, S]), G, test_size=0.33)\n",
    "        self.G_reg = LinearRegression().fit(X_train,y_train)\n",
    "        y_pred    = self.G_reg.predict(X_test)\n",
    "        #print(np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\n",
    "        \n",
    "        self.L_reg = LinearRegression().fit(np.hstack([R, S]), L)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(np.hstack([R, S]), L, test_size=0.33)\n",
    "        self.L_reg = LinearRegression().fit(X_train,y_train)\n",
    "        y_pred    = self.L_reg.predict(X_test)\n",
    "        #print(np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\n",
    "        \n",
    "        G_err = G - self.G_reg.predict(np.hstack([R, S]))\n",
    "        L_err = L - self.L_reg.predict(np.hstack([R, S]))\n",
    "        self.A_reg = PCA(whiten=True, n_components=1).fit(np.hstack([G_err, L_err]))\n",
    "        self.sgn = np.sign(np.corrcoef(self.A_reg.transform(np.hstack([G_err, L_err])).T, G.T)[0, 1])\n",
    "    def evaluate(self, R, S, G, L, nb_seats=None):\n",
    "        assert R.shape == S.shape == G.shape == L.shape\n",
    "        nb_obs = R.shape[0]\n",
    "        if nb_seats is None:\n",
    "            nb_seats = nb_obs\n",
    "        else:\n",
    "            assert isinstance(nb_seats, int) and (nb_seats > 0)\n",
    "            nb_seats = min(nb_obs, nb_seats)\n",
    "        G_err = G - self.G_reg.predict(np.hstack([R, S]))\n",
    "        L_err = L - self.L_reg.predict(np.hstack([R, S]))\n",
    "        A_hat = self.sgn * self.A_reg.transform(np.hstack([G_err, L_err]))\n",
    "        ind = A_hat.argsort(axis=0)[-nb_seats:][::-1]\n",
    "        P = np.zeros([nb_obs, 1]).astype(bool)\n",
    "        P[ind] = True\n",
    "        return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility Functions\n",
    "def normalize(x):\n",
    "    return (x - np.mean(x)) / np.std(x)\n",
    "\n",
    "def minmax_normalizer(df):\n",
    "    return (df - df.min()) / (df.max() - df.min())\n",
    "def build_plot(P, A, R, S, G, L, F, colors=None, pc_samps=1000, figscale=6, fontsize=20):\n",
    "    if colors is None:\n",
    "        colors = {\n",
    "            (0, 0): [(0.882, 0.529, 0.000, 1.000), (1.000, 0.647, 0.000, 0.500)],\n",
    "            (1, 0): [(0.882, 0.000, 0.000, 1.000), (1.000, 0.000, 0.000, 0.500)],\n",
    "            (0, 1): [(0.000, 0.882, 0.000, 1.000), (0.000, 1.000, 0.000, 0.500)],\n",
    "            (1, 1): [(0.000, 0.000, 0.882, 1.000), (0.000, 0.000, 1.000, 0.500)]\n",
    "        }\n",
    "    gs = GridSpec(3, 4)\n",
    "    gs.update(wspace=0, hspace=0)\n",
    "    kwargs_hist = dict(bins=25, histtype='stepfilled', stacked=True)\n",
    "    kwargs_text = dict(horizontalalignment='left', verticalalignment='top', fontsize=fontsize)\n",
    "    fig = plt.figure(figsize=(4 * figscale, 3 * figscale))\n",
    "    ax_dict = dict()\n",
    "    for i, tup in enumerate(itertools.product([0, 1], [0, 1])):\n",
    "        j, k = tup\n",
    "        ind = (R == j) & (S == k)\n",
    "        ax = fig.add_subplot(gs[j, k]) \n",
    "        ax.hist([A[ind & P], A[ind & ~P]], color=colors[tup], **kwargs_hist)\n",
    "        ax.axvline(x=0, ls='dotted', color='black')\n",
    "        ax.text(0.02, 0.98, 'R={0:}, S={1:}'.format(j, k), transform=ax.transAxes, **kwargs_text)\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlim([-5, 5])\n",
    "        ax.set_xticks([])\n",
    "        ax_dict[i] = ax\n",
    "    ylim = [0, 1.05 * max([ax.get_ylim()[1] for ax in ax_dict.values()])]\n",
    "    [ax.set_ylim(ylim) for ax in ax_dict.values()];\n",
    "    ax = fig.add_subplot(gs[0:2, 2:])\n",
    "    ax.hist([A[P], A[~P]], color=['darkgray', 'lightgray'], **kwargs_hist)\n",
    "    ax.axvline(x=0, ls='dotted', color='black')\n",
    "    ax.text(0.01, 0.99, 'All', transform=ax.transAxes, **kwargs_text)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlim([-5, 5])\n",
    "    ax.set_xticks([])\n",
    "    ax = fig.add_subplot(gs[2:, 0:])\n",
    "    z = ['A', 'G', 'L', 'F']\n",
    "    x = range(len(z))\n",
    "    df = pd.DataFrame({'A': A.flat, 'G': G.flat, 'L': L.flat, 'F': F.flat}, columns=z)\n",
    "    df = minmax_normalizer(df)\n",
    "    idx = np.random.choice(range(len(df)), pc_samps)\n",
    "    colors = pd.DataFrame({'R': R.flat, 'S': S.flat}, columns=['R', 'S'])\\\n",
    "      .apply(tuple, axis=1).apply(lambda i: colors[i])\n",
    "    for i in df.index[idx]:\n",
    "        color = colors[i][0] if P[i] else colors[i][1]\n",
    "        alpha = 0.500 if P[i] else 0.025\n",
    "        ax.plot(x, df.loc[i], ls='solid', color=color, alpha=alpha)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_xlim([x[0], x[-1]])\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(z)\n",
    "    [ax.axvline(x=_x, lw=1, ls='dotted', color='black') for _x in x];\n",
    "    for tick in ax.yaxis.get_major_ticks():\n",
    "        tick.label1On = False\n",
    "        tick.tick1On = False\n",
    "    for tick in ax.xaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(fontsize) \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "nb_seats = 20000\n",
    "#Main\n",
    "# set up naive policy\n",
    "naivePolicy = NaivePolicy()\n",
    "\n",
    "# set up and train unaware policy\n",
    "unawarePolicy = UnawarePolicy()\n",
    "unawarePolicy.train(G, L, F)\n",
    "\n",
    "# set up and train fair policy\n",
    "fairPolicy = FairPolicy()\n",
    "fairPolicy.train(Nr, Ns, G, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'S' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ca7a3db59fe3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m'naive'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnaivePolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_seats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;34m'unaware'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0munawarePolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_seats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;34m'fair'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfairPolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_seats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m }\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#build_plot(P['naive'], A, R, S, G, L, F);\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'S' is not defined"
     ]
    }
   ],
   "source": [
    "P = {\n",
    "    'naive': naivePolicy.evaluate(G, L, nb_seats),\n",
    "    'unaware': unawarePolicy.evaluate(G, L, nb_seats),\n",
    "    'fair': fairPolicy.evaluate(S, G, L, nb_seats)\n",
    "}\n",
    "#build_plot(P['naive'], A, R, S, G, L, F);\n",
    "#build_plot(P['unaware'], A, R, S, G, L, F);\n",
    "#build_plot(P['fair'], A, R, S, G, L, F);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
